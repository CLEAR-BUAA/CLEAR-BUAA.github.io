<head>
    <title>ProjectName</title>
    <!-- Meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Yunchao Wei's homepage">
    <link rel="shortcut icon" href="../images/log.png">

    <link href='https://fonts.googleapis.com/css?family=Roboto:400,500,400italic,300italic,300,500italic,700,700italic,900,900italic' rel='stylesheet' type='text/css'>
    <!-- Global CSS -->
    <link rel="stylesheet" href="../css/bootstrap.min.css">
	<link rel="stylesheet" href="../css/bootstrap-responsive.min.css">
	<link rel="stylesheet" href="../css/font-awesome/css/font-awesome.min.css">

    <link rel="stylesheet" href="../css/main.css">

    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->

</head>
<body>
<div class="container">
       <div class="projectheader">
			<h2>ScribbleSup: Scribble-Supervised Convolutional Networks for Semantic Segmentation</h2>
			<a href="http://andelin.github.io/">Di Lin</a><sup>1</sup>*&nbsp;&nbsp;&nbsp;&nbsp;
			<a href="http://research.microsoft.com/en-us/people/jifdai/">Jifeng Dai</a><sup>2</sup>&nbsp;&nbsp;&nbsp;&nbsp;
			<a href="http://www.cse.cuhk.edu.hk/~leojia/">Jiaya Jia</a><sup>1</sup>&nbsp;&nbsp;&nbsp;&nbsp;
			<a href="http://research.microsoft.com/en-us/um/people/kahe/">Kaiming He</a><sup>2</sup>&nbsp;&nbsp;&nbsp;&nbsp;
			<a href="http://research.microsoft.com/en-us/people/jiansun/">Jian Sun</a><sup>2</sup>&nbsp;&nbsp;&nbsp;&nbsp;
			<p><sup>1</sup>The Chinese Univeristy of Hong Kong&nbsp;&nbsp;&nbsp;&nbsp;<sup>2</sup>Microsoft Research</p>
			<p>(*This work was done when Di Lin was an intern at Microsoft Research)</p>
       </div>

		<h4> Introduction </h4>
		<p>
			Large-scale data is of crucial importance for learning semantic segmentation models, but annotating per-pixel masks
			is a tedious and inefficient procedure. We note that for the topic of interactive image segmentation, scribbles are
			very widely used in academic research and commercial software, and are recognized as one of the most user-friendly
			ways of interacting. In this paper, we propose to use scribbles to annotate images, and develop an algorithm to train
			convolutional networks for semantic segmentation supervised by scribbles. Our algorithm is based on a graphical model
			that jointly propagates information from scribbles to unmarked pixels and learns network parameters. We present competitive
			object semantic segmentation results on the PASCAL VOC dataset [1] by using scribbles as annotations. Scribbles are also favored
			for annotating stuff (e.g., water, sky, grass) that has no well-defined shape, and our method shows excellent results on the
			PASCAL-CONTEXT dataset [2] thanks to extra inexpensive scribble annotations.
		</p>

		<h4>PASCAL-Scribble Dataset</h4>
		<p>
			We provide scribble annotations on the PASCAL dataset [1]. Our annotations follow two
			different protocols. In the first protocol, we annotate the PASCAL VOC 2012 set that involves 20
			object categories (aeroplane, bicycle, ...) and one background category. There are 12,031 images annotated,
			including 10,582 images in the training set and 1,449 images in the validation set. The following are
			examples of annotated scribbles on the PASCAL VOC 2012 set.
		</p>

		<div class="projectimg">
			<img alt="" src="../images/test.png"></td>
		</div>

		<p>
			In the second protocol, we follow the 59 object/stuff categories and one background
			category involved in the PASCAL-CONTEXT dataset [2]. Besides the 20 object categories in the first protocol, there are
			39 extra categories (snow, tree, ...) included. We follow this protocol to annotate the PASCAL-CONTEXT dataset.
			We have 4,998 images in the training set annotated. The following are examples of annotated scribbles on the
			PASCAL-CONTEXT dataset.
		</p>

			<div class="projectimg">
				<img alt="" src="../images/test.png"></td>
			</div>
		<p>
			Using the second protocol, we further annotate the 9,963 images in the PASCAL VOC 2007 set.
			The following are examples of annotated scribbles on the PASCAL VOC 2007 set.
		</p>

		   <div class="projectimg">
				<img alt="" src="../images/test.png"></td>
		   </div>

		<hr>
		<h4>Downloads</h4>

			<a href="http://arxiv.org/abs/1604.05144"> "ScribbleSup: Scribble-Supervised Convolutional Networks for Semantic Segmentation &quot; </a>
			<br>
			Di Lin, Jifeng Dai, Jiaya Jia, Kaiming He, and Jian Sun
			<br>
			IEEE Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2016
			<br>
			<br>
			<p>Please download the original images from the <a href="http://host.robots.ox.ac.uk/pascal/VOC/"> <b>PASCAL VOC website</b></a>.</p>


		<h4>Reference</h4>
		<p>
			[1] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zisserman. The PASCAL Visual Object Classes (VOC)
				Challenge. IJCV, 2010.
		</p>
		<p>
			[2] R. Mottaghi, X. Chen, X. Liu, N.-G. Cho, S.-W. Lee, S. Fidler, R. Urtasun, and A. Yuille. The role of context
				for object detection and semantic segmentation in the wild. In CVPR, 2014.
		</p>
</div>
</body>

</html>
